{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "REPO_DIR = \"../dinov3\"\n",
    "segmentor_head_dir = \"./models/dinov3_vit7b16_ade20k_m2f_head-bf307cb1.pth\"\n",
    "backbone_checkpoint_dir = \"\"\n",
    "\n",
    "sys.path.append(REPO_DIR)\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from functools import partial\n",
    "from dinov3.eval.segmentation.inference import make_inference\n",
    "\n",
    "\n",
    "def get_img():\n",
    "    import requests\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def make_transform(resize_size: int | list[int] = 768):\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    resize = transforms.Resize((resize_size, resize_size), antialias=True)\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=(0.485, 0.456, 0.406),\n",
    "        std=(0.229, 0.224, 0.225),\n",
    "    )\n",
    "    return transforms.Compose([to_tensor, resize, normalize])\n",
    "\n",
    "segmentor = torch.hub.load(REPO_DIR, 'dinov3_vit7b16_ms', source=\"local\", weights=<SEGMENTOR/CHECKPOINT/URL/OR/PATH>, backbone_weights=<BACKBONE/CHECKPOINT/URL/OR/PATH>)\n",
    "\n",
    "img_size = 896\n",
    "img  = get_img()\n",
    "transform = make_transform(img_size)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "        batch_img = transform(img)[None]\n",
    "        pred_vit7b = segmentor(batch_img)  # raw predictions\n",
    "        # actual segmentation map\n",
    "        segmentation_map_vit7b = make_inference(\n",
    "            batch_img,\n",
    "            segmentor,\n",
    "            inference_mode=\"slide\",\n",
    "            decoder_head_type=\"m2f\",\n",
    "            rescale_to=(img.size[-1], img.size[-2]),\n",
    "            n_output_channels=150,\n",
    "            crop_size=(img_size, img_size),\n",
    "            stride=(img_size, img_size),\n",
    "            output_activation=partial(torch.nn.functional.softmax, dim=1),\n",
    "        ).argmax(dim=1, keepdim=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(segmentation_map_vit7b[0,0].cpu(), cmap=colormaps[\"Spectral\"])\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dinov3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
